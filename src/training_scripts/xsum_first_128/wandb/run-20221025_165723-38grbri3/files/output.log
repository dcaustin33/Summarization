Starting from step 0
Training for 20000 steps
10 19.595283269882202
20 36.078834533691406
30 53.00361919403076
40 69.78377437591553
50 86.52614378929138
60 102.84300112724304
70 119.45905375480652
80 136.39074635505676
90 152.90366339683533
100 Loss: 443.64
In Logging <wandb.sdk.wandb_run.Run object at 0x7f9fd8b938d0>
logging
100 169.5987572669983
/opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  UserWarning,
out val

110 209.53116011619568
120 225.94657039642334
130 242.37020564079285
140 258.7998492717743
150 275.444748878479
160 292.2228488922119
170 308.63600993156433
180 325.3487265110016
190 342.16706132888794
200 Loss: 408.94
In Logging <wandb.sdk.wandb_run.Run object at 0x7f9fd8b938d0>
logging
200 358.87560296058655
Traceback (most recent call last):
  File "train_xsum.py", line 179, in <module>
    trainer.train()
  File "/home/da2986/Summarization/src/training_scripts/trainer.py", line 114, in train
    loss = self.validation_step(val_data, self.model, self.val_metrics, steps, log = False, wandb = self.wandb, args = self.args)
  File "train_xsum.py", line 62, in validation_step
    generate_out = model.generate(input_ids = data['article']['input_ids'], attention_mask = data['article']['attention_mask'])
  File "/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py", line 1466, in generate
    **model_kwargs,
  File "/opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py", line 2304, in beam_search
    output_hidden_states=output_hidden_states,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1416, in forward
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1270, in forward
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1103, in forward
    use_cache=use_cache,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 430, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 209, in forward
    key_states = torch.cat([past_key_value[0], key_states], dim=2)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.17 GiB total capacity; 10.40 GiB already allocated; 23.25 MiB free; 10.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF