{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/da2986/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Found cached dataset xsum (/home/da2986/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rouge import Rouge\n",
    "import transformers\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import transformers\n",
    "from trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "import wandb\n",
    "from logger import log_metrics\n",
    "#import gradient checkpointing\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class XSumDatasetBERT(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_name = 'google/pegasus-large', max_length=256, split = 'train'):\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.max_length = max_length\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.dataset = load_dataset(\"xsum\", split = split)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_bert_embeddings(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        embeddings = self.model(**inputs)['pooler_output']\n",
    "        #compute cosine similarity between embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['document']\n",
    "        text = text.split('.')\n",
    "        text = [i.strip() for i in text]\n",
    "        \n",
    "        embeddings = self.get_bert_embeddings(text)\n",
    "        first = np.random.choice(len(text), 1, replace=False)[0]\n",
    "        chosen_embeddings = torch.empty((1, 768)).cuda()\n",
    "        bag_of_sentences = [first]\n",
    "        chosen_embeddings[0] = embeddings[first]\n",
    "        current_size = len(text[first])\n",
    "\n",
    "        while current_size < self.max_length:\n",
    "            new_cosine_sim = torch.mm(chosen_embeddings, embeddings.T)\n",
    "            vals, indices = torch.topk(-torch.sum(new_cosine_sim, dim = 0), k = len(text))\n",
    "            for i in indices:\n",
    "                if i not in bag_of_sentences:\n",
    "                    chosen_embeddings = torch.cat((chosen_embeddings, embeddings[i].unsqueeze(0)), dim = 0)\n",
    "                    bag_of_sentences.append(i.item())\n",
    "                    current_size += len(text[i])\n",
    "                    break\n",
    "\n",
    "        bag_of_sentences = sorted(bag_of_sentences)\n",
    "        final = [text[i] for i in bag_of_sentences]\n",
    "        final = '. '.join(final)\n",
    "\n",
    "        summary_text = self.dataset[idx]['summary']\n",
    "        return {'article_text':final, 'summary_text': summary_text}\n",
    "\n",
    "class XSumDatasetPowerLaw(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_name = 'google/pegasus-large', max_length=256, split = 'train', divisor = 2):\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.max_length = max_length\n",
    "        self.dataset = load_dataset(\"xsum\", split = split)\n",
    "        self.max_length = max_length\n",
    "        self.probability = np.ones(1000) * 1000000\n",
    "        for i, val in enumerate(self.probability):\n",
    "            if i == 0: continue\n",
    "            self.probability[i] = self.probability[i-1] / divisor\n",
    "        self.indexes = np.arange(1000)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['document']\n",
    "        text = text.split('.')\n",
    "        \n",
    "        max_idx = max(1, len(text))\n",
    "        choices = np.random.choice(self.indexes[:max_idx], max_idx, replace = False, p = self.probability[:max_idx] / self.probability[:max_idx].sum())\n",
    "\n",
    "        current_size = 0\n",
    "        counter = 0\n",
    "        while current_size < self.max_length and counter < max_idx:\n",
    "            current_size += len(text[choices[counter]])\n",
    "            counter += 1\n",
    "\n",
    "        choices = sorted(choices[:counter])\n",
    "        final = list(np.array(text)[choices])\n",
    "        text = '. '.join(final)\n",
    "\n",
    "        summary_text = self.dataset[idx]['summary']\n",
    "        return {'article_text':text, 'summary_text': summary_text}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset1 = XSumDatasetPowerLaw()\n",
    "dataset2 = XSumDatasetBERT()\n",
    "dataloader = torch.utils.data.DataLoader(dataset1, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_text': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. \\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. \\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit',\n",
       " 'summary_text': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_text': '\"That may not be true but it is perhaps my perspective over the last few days. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. \"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses. uk. ',\n",
       " 'summary_text': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The US says a purported confession from a prominent Chinese lawyer on state television runs counter to the rule of law.', 'The proportion of men taking their own lives in the UK has reached its highest level for more than a decade, according to official figures.']\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    #print(data['article_text'])\n",
    "    print(data['summary_text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence['article_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. \"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses. Have you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. co\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "embeddings = model(**inputs)['pooler_output']\n",
    "#compute cosine similarity between embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "first = np.random.choice(len(sentence), 1, replace=False)\n",
    "\n",
    "new_embeddings = torch.empty((1, 768)).cuda()\n",
    "bag_of_sentences = [first[0]]\n",
    "new_embeddings[0] = embeddings[first]\n",
    "current_size = len(sentence[first[0]])\n",
    "\n",
    "while current_size < 500:\n",
    "    new_cosine_sim = torch.mm(new_embeddings, embeddings.T)\n",
    "    vals, idx = torch.topk(-torch.sum(new_cosine_sim, dim = 0), k = len(sentence))\n",
    "    for i in idx:\n",
    "        if i not in bag_of_sentences:\n",
    "            new_embeddings = torch.cat((new_embeddings, embeddings[i].unsqueeze(0)), dim = 0)\n",
    "            bag_of_sentences.append(i.item())\n",
    "            current_size += len(sentence[i])\n",
    "            break\n",
    "\n",
    "bag_of_sentences = sorted(bag_of_sentences)\n",
    "final = [sentence[i] for i in bag_of_sentences]\n",
    "final = '. '.join(final)\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  5 15:56:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0    61W / 149W |   7798MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2066      C   ...2986/anaconda3/bin/python     7795MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "998174799bccd60bb63e033f702b872724459890f16847813c212c177da2da49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
